{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project Notebook\n",
    "\n",
    "This notebook contains the main analysis and experiments for the NLP final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST... Follow instructions in README.md\n",
    "\n",
    "Check if Ollama is properly installed and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./scripts/check_ollama.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Example \n",
    "\n",
    "Below is an example cell on how to reference and call functions we create in isolated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.utils import example_plot, example_function\n",
    "\n",
    "example_plot()\n",
    "example_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Custom Imports\n",
    "from src.io.ollama_client import OllamaClient\n",
    "from src.io.ollama_client import ConversationEntry, SavedConversationData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OllamaClient Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the client that wraps the usage of the ollama API, and should be instantiated per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = OllamaClient(\"qwen3:0.6b\", log_conversations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate text from the model, this will generate a standalone response (no context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world! ðŸ˜Š How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen.generate_text(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, the conversation history will be populated with the prompt and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[{'input': 'Hello, world!', 'response': 'Hello, world! ðŸ˜Š How can I assist you today?', 'details': {'model': 'qwen3:0.6b', 'created_at': '2025-10-31T18:32:40.67302Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8326420959, 'load_duration': 7542218917, 'prompt_eval_count': 14, 'prompt_eval_duration': 323743167, 'eval_count': 86, 'eval_duration': 425697880, 'response': 'Hello, world! ðŸ˜Š How can I assist you today?', 'thinking': 'Okay, the user said \"Hello, world!\" so I should respond with a friendly greeting. Let me make sure to keep it simple and positive. Maybe say \"Hello, world!\" and add a friendly message. Let me check if there\\'s any specific context needed, but since it\\'s a general greeting, keeping it straightforward is best.\\n', 'context': [151644, 872, 198, 9707, 11, 1879, 0, 608, 26865, 151645, 198, 151644, 77091, 198, 151667, 198, 32313, 11, 279, 1196, 1053, 330, 9707, 11, 1879, 8958, 773, 358, 1265, 5889, 448, 264, 11657, 42113, 13, 6771, 752, 1281, 2704, 311, 2506, 432, 4285, 323, 6785, 13, 10696, 1977, 330, 9707, 11, 1879, 8958, 323, 912, 264, 11657, 1943, 13, 6771, 752, 1779, 421, 1052, 594, 894, 3151, 2266, 4362, 11, 714, 2474, 432, 594, 264, 4586, 42113, 11, 10282, 432, 30339, 374, 1850, 624, 151668, 271, 9707, 11, 1879, 0, 26525, 232, 2585, 646, 358, 7789, 498, 3351, 30]}, 'timestamp': '2025-10-31T14:32:40.687687'}]\n",
      "{'input': 'Hello, world!', 'response': 'Hello, world! ðŸ˜Š How can I assist you today?', 'details': {'model': 'qwen3:0.6b', 'created_at': '2025-10-31T18:32:40.67302Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8326420959, 'load_duration': 7542218917, 'prompt_eval_count': 14, 'prompt_eval_duration': 323743167, 'eval_count': 86, 'eval_duration': 425697880, 'response': 'Hello, world! ðŸ˜Š How can I assist you today?', 'thinking': 'Okay, the user said \"Hello, world!\" so I should respond with a friendly greeting. Let me make sure to keep it simple and positive. Maybe say \"Hello, world!\" and add a friendly message. Let me check if there\\'s any specific context needed, but since it\\'s a general greeting, keeping it straightforward is best.\\n', 'context': [151644, 872, 198, 9707, 11, 1879, 0, 608, 26865, 151645, 198, 151644, 77091, 198, 151667, 198, 32313, 11, 279, 1196, 1053, 330, 9707, 11, 1879, 8958, 773, 358, 1265, 5889, 448, 264, 11657, 42113, 13, 6771, 752, 1281, 2704, 311, 2506, 432, 4285, 323, 6785, 13, 10696, 1977, 330, 9707, 11, 1879, 8958, 323, 912, 264, 11657, 1943, 13, 6771, 752, 1779, 421, 1052, 594, 894, 3151, 2266, 4362, 11, 714, 2474, 432, 594, 264, 4586, 42113, 11, 10282, 432, 30339, 374, 1850, 624, 151668, 271, 9707, 11, 1879, 0, 26525, 232, 2585, 646, 358, 7789, 498, 3351, 30]}, 'timestamp': '2025-10-31T14:32:40.687687'}\n"
     ]
    }
   ],
   "source": [
    "print(qwen.get_conversation_count())\n",
    "# The conversation history can be accessed by calling the get_conversation_history method\n",
    "print(qwen.get_conversation_history())\n",
    "# The last conversation can be accessed by calling the get_last_conversation method\n",
    "print(qwen.get_last_conversation())\n",
    "# The conversation history can be cleared by calling the clear_conversation_history method\n",
    "# qwen.clear_conversation_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Persistence\n",
    "\n",
    "You can save and load conversation histories to disk for experiment tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Machine learning is like helping computers learn from data to make better guesses. For example, if you look at a picture of a cat, your computer learns to recognize it from that image.\n",
      "\n",
      "Response 2: A neural network is like a brain made of tiny circuits that help learn and make decisions.\n",
      "\n",
      "Saved conversation history to: experiments/test_experiment/conversation_history_20251031_143242.json\n"
     ]
    }
   ],
   "source": [
    "# Create a client with custom output directory\n",
    "client = OllamaClient(\"qwen3:0.6b\", output_dir=\"./experiments\", log_conversations=True)\n",
    "\n",
    "# Generate some conversations\n",
    "response1 = client.generate_text(\"What is machine learning? Explain like I'm 5 in 20 words or less.\")\n",
    "print(f\"Response 1: {response1}\\n\")\n",
    "\n",
    "response2 = client.generate_text(\"Explain neural networks? Explain like I'm 5 in 20 words or less.\")\n",
    "print(f\"Response 2: {response2}\\n\")\n",
    "\n",
    "# Flush conversation history to disk\n",
    "saved_path = client.flush_conversation_history(\"test_experiment\")\n",
    "print(f\"Saved conversation history to: {saved_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Conversation History\n",
    "\n",
    "You can reload conversation histories from disk. By default, it loads the most recent file.\n",
    "\n",
    "- Method 1: Using get_conversation_history() to iterate through the in-memory loaded conversations\n",
    "- Method 2: Using iter_experiment_conversations() to iterate directly from files (memory-efficient for large experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 conversations from disk\n",
      "\n",
      "Conversation count: 8\n",
      "Last response: A neural network is like a brain made of tiny circuits that help learn and make decisions.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_client = OllamaClient(\"qwen3:0.6b\", output_dir=\"./experiments\", log_conversations=True)\n",
    "\n",
    "# Load the most recent conversation history for \"test_experiment\"\n",
    "count = new_client.load_conversation_history(\"test_experiment\")\n",
    "print(f\"Loaded {count} conversations from disk\\n\")\n",
    "\n",
    "# Verify the loaded history\n",
    "print(f\"Conversation count: {new_client.get_conversation_count()}\")\n",
    "print(f\"Last response: {new_client.get_last_response_text()}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Method 1: Iterate through in-memory conversation history\n",
    "# print(\"METHOD 1: Iterating through in-memory conversation history\")\n",
    "# print(\"=\"*80)\n",
    "# for i, conv in enumerate[ConversationEntry](new_client.get_conversation_history(), 1):\n",
    "#     print(f\"\\n--- Conversation {i} ---\")\n",
    "#     print(f\"Prompt:   {conv['input'][:80]}...\")  # First 80 chars\n",
    "#     print(f\"Response: {conv['response'][:80]}...\")  # First 80 chars\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Method 2: Memory-efficient file iterator (doesn't load all into memory)\n",
    "# print(\"METHOD 2: Iterating using file iterator (memory-efficient)\")\n",
    "# print(\"=\"*80)\n",
    "# for i, conv in enumerate[ConversationEntry](new_client.iter_experiment_conversations(\"test_experiment\"), 1):\n",
    "#     print(f\"\\n--- Conversation {i} ---\")\n",
    "#     print(f\"Prompt:   {conv['input'][:80]}...\")  # First 80 chars\n",
    "#     print(f\"Response: {conv['response'][:80]}...\")  # First 80 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Saved JSON Structure\n",
    "\n",
    "The saved conversation history contains all the details needed for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Structure:\n",
      "Input: What is machine learning? Explain like I'm 5 in 20 words or less.\n",
      "Response: Machine learning is when computers learn from data to make smart decisions. Like a kid learning to play games by watching others.\n",
      "Type: generate\n",
      "Timestamp: 2025-10-31T14:12:20.397531\n",
      "\n",
      "Details keys: dict_keys(['model', 'created_at', 'done', 'done_reason', 'total_duration', 'load_duration', 'prompt_eval_count', 'prompt_eval_duration', 'eval_count', 'eval_duration', 'response', 'thinking', 'context'])\n"
     ]
    }
   ],
   "source": [
    "# Example of viewing a single conversation entry\n",
    "conversation = new_client.get_conversation_history()[0]\n",
    "\n",
    "print(\"Conversation Structure:\")\n",
    "print(f\"Input: {conversation['input']}\")\n",
    "print(f\"Response: {conversation['response']}\")\n",
    "print(f\"Type: {conversation['type']}\")\n",
    "print(f\"Timestamp: {conversation['timestamp']}\")\n",
    "print(f\"\\nDetails keys: {conversation['details'].keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from ollama and stored in the client contains a ton of information if needed. Note the difference in the already parsed thinking and response secitons, this could be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model\": \"qwen3:0.6b\",\n",
      "    \"created_at\": \"2025-10-31T18:32:40.67302Z\",\n",
      "    \"done\": true,\n",
      "    \"done_reason\": \"stop\",\n",
      "    \"total_duration\": 8326420959,\n",
      "    \"load_duration\": 7542218917,\n",
      "    \"prompt_eval_count\": 14,\n",
      "    \"prompt_eval_duration\": 323743167,\n",
      "    \"eval_count\": 86,\n",
      "    \"eval_duration\": 425697880,\n",
      "    \"response\": \"Hello, world! \\ud83d\\ude0a How can I assist you today?\",\n",
      "    \"thinking\": \"Okay, the user said \\\"Hello, world!\\\" so I should respond with a friendly greeting. Let me make sure to keep it simple and positive. Maybe say \\\"Hello, world!\\\" and add a friendly message. Let me check if there's any specific context needed, but since it's a general greeting, keeping it straightforward is best.\\n\",\n",
      "    \"context\": [\n",
      "        151644,\n",
      "        872,\n",
      "        198,\n",
      "        9707,\n",
      "        11,\n",
      "        1879,\n",
      "        0,\n",
      "        608,\n",
      "        26865,\n",
      "        151645,\n",
      "        198,\n",
      "        151644,\n",
      "        77091,\n",
      "        198,\n",
      "        151667,\n",
      "        198,\n",
      "        32313,\n",
      "        11,\n",
      "        279,\n",
      "        1196,\n",
      "        1053,\n",
      "        330,\n",
      "        9707,\n",
      "        11,\n",
      "        1879,\n",
      "        8958,\n",
      "        773,\n",
      "        358,\n",
      "        1265,\n",
      "        5889,\n",
      "        448,\n",
      "        264,\n",
      "        11657,\n",
      "        42113,\n",
      "        13,\n",
      "        6771,\n",
      "        752,\n",
      "        1281,\n",
      "        2704,\n",
      "        311,\n",
      "        2506,\n",
      "        432,\n",
      "        4285,\n",
      "        323,\n",
      "        6785,\n",
      "        13,\n",
      "        10696,\n",
      "        1977,\n",
      "        330,\n",
      "        9707,\n",
      "        11,\n",
      "        1879,\n",
      "        8958,\n",
      "        323,\n",
      "        912,\n",
      "        264,\n",
      "        11657,\n",
      "        1943,\n",
      "        13,\n",
      "        6771,\n",
      "        752,\n",
      "        1779,\n",
      "        421,\n",
      "        1052,\n",
      "        594,\n",
      "        894,\n",
      "        3151,\n",
      "        2266,\n",
      "        4362,\n",
      "        11,\n",
      "        714,\n",
      "        2474,\n",
      "        432,\n",
      "        594,\n",
      "        264,\n",
      "        4586,\n",
      "        42113,\n",
      "        11,\n",
      "        10282,\n",
      "        432,\n",
      "        30339,\n",
      "        374,\n",
      "        1850,\n",
      "        624,\n",
      "        151668,\n",
      "        271,\n",
      "        9707,\n",
      "        11,\n",
      "        1879,\n",
      "        0,\n",
      "        26525,\n",
      "        232,\n",
      "        2585,\n",
      "        646,\n",
      "        358,\n",
      "        7789,\n",
      "        498,\n",
      "        3351,\n",
      "        30\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "resp = qwen.get_last_conversation().get(\"details\")\n",
    "print(json.dumps(resp, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world! ðŸ˜Š How can I assist you today?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen.get_last_response_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how you choose to setup your experiments, you can choose to use the cached responses in the client class, or keep track of the inputs and outputs outside of the client, either is fine just use `log_conversations=False` to prevent memory bloat if you are doing it yourself. I imagine keeping it within the class will work better, then appending a new message like `prompt + qwen.get_last_response()` for the output test. This way, we can parse the message history into formats used for calculating embeddings, drift, accelertaion etc. Use the flush functionality to save to the file system if you are running this for a while, or running this for a lot of iterations. The scheme for your tests should be split into data creation, then data parsing. I.E. create a cell to run the prompts and save the conversations, then create a cell that iterates through the class or files to compute embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings using the same OllamaClient with a new model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "[-1.62469430e-01  3.26936550e-03  3.11880990e-02 -1.59651330e-03\n",
      " -6.53016600e-03 -5.89228760e-04 -4.31132000e-02  4.79441320e-02\n",
      "  3.38893720e-02 -4.58842550e-02 -1.49056690e-02 -4.48386070e-02\n",
      "  4.63952160e-02 -7.78193030e-03  1.11231430e-01  5.02321700e-03\n",
      "  3.52546480e-03 -5.01822270e-02 -8.63471500e-02  3.89924040e-03\n",
      "  5.12065140e-02 -3.44341060e-02  3.50343300e-03 -3.93365250e-02\n",
      "  2.87021960e-02  4.58670850e-02 -1.80611050e-02 -1.86819150e-02\n",
      " -8.57127100e-03  1.64970900e-03  3.30678000e-02 -7.02537130e-03\n",
      "  4.23321000e-02 -8.51234050e-03  2.30838040e-02  3.57760820e-02\n",
      "  1.63466930e-02 -7.73056450e-02  6.65985940e-02 -2.67599580e-02\n",
      " -9.51105800e-02  6.23018300e-02 -1.02202930e-02 -3.44547300e-02\n",
      " -2.88139100e-03 -4.41396120e-02 -5.94716260e-02 -5.43507800e-02\n",
      " -2.22388640e-02  2.93579340e-02 -1.48777290e-02  6.02593970e-03\n",
      " -2.71349080e-02  1.94219720e-02 -3.81459780e-02 -6.89787320e-03\n",
      "  4.73572300e-03 -4.33932150e-03 -4.41020400e-02  4.05428260e-02\n",
      " -5.37357480e-02 -5.04777700e-02 -9.02630950e-03 -4.31686360e-03\n",
      "  3.82305000e-02 -1.83735230e-02  2.08168240e-03  2.80384280e-03\n",
      "  4.71721920e-02  2.72836480e-01 -4.08025160e-02 -3.30397370e-02\n",
      " -1.65491220e-02 -5.21847050e-02  2.43127550e-01  5.57859100e-02\n",
      " -8.01190700e-03 -1.59699700e-02 -4.60276900e-02  2.92015040e-02\n",
      "  4.53489040e-03  7.87221900e-03  6.34551160e-04 -2.42324000e-02\n",
      "  8.88685900e-02 -1.98365600e-02 -3.00086590e-02 -1.62068360e-02\n",
      "  4.88077100e-02 -3.57044940e-02 -1.26797170e-02 -6.14574130e-03\n",
      " -1.63757170e-02  1.65619070e-02 -1.38304790e-03 -4.81850840e-02\n",
      "  2.78329660e-03  5.87606130e-02  2.92490180e-02  1.01378480e-02\n",
      "  2.63140540e-02 -3.04777760e-03  5.50216660e-02  7.45774500e-02\n",
      "  3.47136000e-02  2.50798870e-02 -1.53692950e-02 -1.69727130e-02\n",
      " -3.19730160e-02  6.19779570e-03 -5.77075780e-02 -8.84512300e-03\n",
      " -1.27491730e-03 -3.73521040e-02 -6.10480830e-03  6.50982300e-03\n",
      " -3.20953540e-02 -5.93751160e-03 -1.68597060e-03  9.06504400e-04\n",
      "  4.80124840e-02 -1.22271420e-02  6.23403400e-03 -3.68892500e-02\n",
      "  2.32785530e-02  3.05401500e-02  2.25464400e-02  5.65024560e-03\n",
      " -2.72515210e-02 -1.63295360e-02  1.23494850e-02  2.62139770e-02\n",
      " -1.06619650e-02  2.41472610e-02 -1.00262950e-03 -1.82758600e-02\n",
      " -2.40525660e-02  3.47970430e-03  1.11155010e-01  1.15480260e-02\n",
      " -7.52076130e-03 -7.52888250e-02 -2.67966900e-02 -1.41814560e-02\n",
      " -7.54877650e-03  1.30600035e-02 -6.39983100e-02 -4.71393240e-02\n",
      "  2.04187740e-02 -3.87863440e-03  1.68832900e-02 -8.58579500e-03\n",
      "  3.88921570e-02 -4.54399970e-03 -1.95042200e-02 -1.21713860e-02\n",
      " -3.19298300e-02  6.76185030e-03 -7.23758640e-02 -5.16442000e-02\n",
      " -3.57054770e-02 -1.37749580e-02  4.51364780e-02  5.15358820e-02\n",
      "  9.41258000e-03  6.34107140e-02  2.76623520e-02  4.23003800e-02\n",
      " -3.28201300e-02 -3.47193800e-02 -2.32216530e-03 -4.19807880e-02\n",
      "  1.02283250e-02 -4.73278800e-02 -5.71166240e-02 -1.87224410e-02\n",
      "  2.85939920e-03 -8.71489400e-03  7.88408150e-02  6.15241100e-02\n",
      " -2.15819550e-02  5.44534440e-02 -3.80048640e-03 -2.68830450e-02\n",
      " -5.07599450e-02  4.82090500e-02  1.27800675e-02  3.41309190e-03\n",
      " -8.55349100e-03 -2.71014500e-02  2.58038450e-02 -1.94189790e-02\n",
      "  3.61234250e-03 -2.35926560e-02 -2.30904060e-03  6.00669870e-02\n",
      "  7.71918400e-02  3.50345000e-02 -2.02784340e-03  1.85079280e-02\n",
      " -9.90358900e-03 -4.73408700e-02 -5.58705960e-02 -7.65799950e-04\n",
      " -5.23925650e-02  7.43601150e-03  1.20534260e-02 -5.12631900e-03\n",
      " -1.63850060e-02  2.94702370e-02  7.20792260e-03  2.74885070e-02\n",
      " -4.15750970e-02 -1.95160100e-02  1.83055680e-02  2.99518500e-02\n",
      " -6.17910250e-02  3.79728320e-03  2.57744580e-02 -3.06430330e-02\n",
      " -1.18977540e-02  4.15176800e-02 -3.32593960e-02 -1.45354320e-02\n",
      " -5.73949780e-02 -8.04470000e-03 -8.54729300e-02 -9.68499000e-03\n",
      "  2.97809750e-02 -1.68114100e-02 -2.79864870e-02  4.40741670e-02\n",
      " -4.96457140e-02  2.04712770e-02 -1.42297230e-02 -3.59686650e-02\n",
      " -2.70735730e-02  4.20563200e-02  5.11498200e-02  6.26448900e-03\n",
      "  3.68627280e-03  8.01399100e-03  2.52725520e-02  3.63480600e-02\n",
      " -2.46453020e-02 -1.95479490e-02  1.76834260e-02 -4.07143050e-02\n",
      " -2.13287840e-03  4.52238100e-03  6.44886350e-03 -5.31550560e-04\n",
      "  5.25878370e-03 -2.54025300e-02 -5.98517670e-03  4.50251440e-02\n",
      "  3.86212600e-02  8.54394000e-03  3.66261150e-02  1.54880590e-02\n",
      "  4.54016740e-02 -1.80081350e-02 -4.34228170e-03 -1.45257550e-04\n",
      "  2.73096820e-02  2.75702770e-02  4.44223730e-02 -7.75745980e-03\n",
      " -9.55269500e-02  3.62387460e-03  7.23169450e-02 -4.69096140e-03\n",
      " -1.62395460e-02 -1.38765730e-02  3.12060000e-02  3.01865230e-02\n",
      " -6.78456100e-02  2.43994100e-03  3.84539970e-03 -1.70808620e-04\n",
      " -1.76608000e-02 -1.64157190e-02  1.09501990e-02  3.61267850e-02\n",
      "  2.39143670e-02 -6.00312040e-03 -2.43129320e-02 -4.18277500e-02\n",
      " -2.23652240e-02 -3.71006100e-02 -4.07650930e-03  3.04038100e-02\n",
      "  3.28430160e-02  2.41810230e-02 -7.87624250e-03  1.69605200e-02\n",
      " -8.18145500e-02  4.66334220e-02 -3.87034100e-02  3.66393960e-02\n",
      " -1.30171780e-01 -3.24228700e-02 -4.89789960e-02 -1.55629660e-02\n",
      "  4.34911400e-02  2.45111770e-02  3.34452730e-02  4.93337000e-02\n",
      "  5.35182700e-03  3.97078900e-04 -2.59516390e-02  3.65752020e-02\n",
      " -6.52763700e-03  2.11943840e-02  2.18133540e-02  1.75788640e-02\n",
      "  1.72142680e-02 -1.28667510e-02  3.23256740e-04  3.43747460e-03\n",
      "  5.61495100e-02 -1.53775720e-02  4.82218700e-02 -7.07962970e-03\n",
      " -2.47254830e-03  6.97568500e-03  7.00079100e-02 -3.48680580e-02\n",
      " -7.54917700e-02  6.11966740e-02 -9.59207800e-03  2.11455150e-02\n",
      "  5.79885400e-02  2.06907130e-02 -3.93888540e-02 -5.29413300e-03\n",
      " -2.19987440e-02 -2.75422800e-02 -1.65490120e-02  1.45716400e-03\n",
      "  2.68986750e-03 -3.32223200e-02  1.36515690e-02  1.65663450e-02\n",
      "  3.64676040e-02  6.61693200e-02 -1.94667410e-02  2.86607020e-02\n",
      " -1.14792830e-02 -7.18313500e-03 -4.33490000e-02 -7.22769040e-03\n",
      " -3.72279620e-03 -2.78736730e-02 -5.56494900e-02 -3.77357160e-02\n",
      "  3.64955780e-02  3.97392600e-02  6.19043560e-02  7.86992200e-03\n",
      "  5.86606260e-02 -1.18168180e-01  4.50729900e-02 -3.17074360e-02\n",
      "  2.52004700e-02 -1.23440360e-02 -1.90551930e-02 -8.51712700e-03\n",
      "  1.70370140e-02  4.31354460e-02  2.61949100e-03 -1.86547700e-02\n",
      "  3.00158490e-02  2.20231800e-02 -8.05212400e-03 -1.93931090e-02\n",
      "  1.95529570e-02 -2.52543240e-02  1.25408100e-02 -1.98924430e-02\n",
      " -3.88005450e-02 -4.72133530e-02 -8.72207200e-03 -1.00386250e-02\n",
      "  4.05606300e-02  1.92600120e-02  1.74935220e-02  4.34625340e-03\n",
      " -2.53023230e-02  2.45435200e-02 -9.67327600e-02  5.46841320e-02\n",
      "  3.14911980e-02 -4.15335130e-06 -1.51434300e-03 -2.60479580e-02\n",
      " -5.24190100e-02  7.42994500e-02 -1.80224460e-02 -2.49898660e-02\n",
      "  1.53429610e-02  2.62054410e-02 -5.04936900e-02  4.16131800e-02\n",
      "  4.70721760e-02 -7.15065900e-02 -2.19098340e-03 -3.24879200e-02\n",
      " -2.44951620e-02  3.92185640e-04  9.22229400e-03 -5.83885800e-02\n",
      " -5.09755500e-02  1.93676460e-02  6.13113460e-04 -6.29347700e-02\n",
      " -3.44237460e-02 -1.23183700e-02 -9.39145000e-02 -2.55329580e-03\n",
      " -4.39444780e-02 -4.90709600e-03  3.36671430e-02  3.67524700e-02\n",
      "  4.33960740e-02  7.70385640e-02  1.82675500e-03 -4.95071480e-03\n",
      "  3.14559040e-03 -1.51863430e-02  3.29212060e-02 -1.84080250e-02\n",
      " -2.12024860e-02  1.89402140e-02  3.79989450e-02 -3.80728300e-02\n",
      " -2.21599960e-02  2.43749920e-02  7.87614300e-03  3.73387300e-02\n",
      " -1.21100780e-02 -3.94828300e-02  1.47134970e-02  2.48334180e-03\n",
      " -1.20832430e-02 -1.57654800e-02  5.25727540e-03  1.37335750e-02\n",
      "  8.88395200e-03 -1.22331060e-02 -5.79592030e-03  2.15828250e-02\n",
      "  1.53274790e-02 -1.06967870e-02 -2.97477580e-02 -4.68519840e-03\n",
      " -5.76971150e-03  9.86950800e-03 -2.02023610e-02 -2.67306440e-02\n",
      "  2.03147400e-03  1.67319100e-02  2.53142170e-02 -3.71653600e-02\n",
      "  3.80396260e-03 -1.38424930e-02 -3.11094620e-02 -2.06249900e-02\n",
      " -5.23306900e-02 -5.85043300e-04 -6.42319500e-02  2.48438440e-02\n",
      "  9.68926100e-04  6.73205500e-02  4.62633600e-03  5.68574000e-03\n",
      "  3.64705600e-04 -2.87284900e-03  3.55948840e-02  3.41917760e-02\n",
      " -3.15101220e-02  1.37159160e-02 -9.94515100e-03  5.32217800e-03\n",
      "  4.98626200e-02 -3.05516900e-02  5.37647120e-02  3.85446660e-02\n",
      "  2.14190330e-02  5.64267770e-03  4.65905870e-05  3.54394700e-02\n",
      "  7.12053600e-03  4.44214640e-02  7.51302620e-03  1.07623460e-02\n",
      " -4.85977870e-02  3.81408040e-02 -4.57933600e-02  1.42120550e-02\n",
      "  7.06541170e-03  2.58135940e-02  3.56196200e-02  2.24471280e-02\n",
      " -1.64528520e-03  5.83159300e-03 -7.68806650e-03 -5.16547800e-02\n",
      " -3.16225440e-02  1.45355740e-02 -8.90580400e-03 -4.79835230e-02\n",
      "  1.78660250e-02  4.20093900e-02  9.40748200e-03 -5.22163960e-02\n",
      " -4.72424330e-02  4.35982420e-02  3.71727720e-02  1.81886420e-02\n",
      "  3.40224400e-02 -3.22721040e-02 -1.21710610e-02  3.85705270e-02\n",
      " -1.49487520e-02  9.62751200e-03 -3.54255960e-02 -2.92073360e-02\n",
      "  4.83740340e-02 -5.38856050e-02  1.88471270e-02  4.26470750e-02\n",
      " -9.36777800e-03 -2.40466580e-02 -2.82346850e-02 -1.99697840e-02\n",
      "  1.68401110e-03 -2.75536530e-02  8.93270000e-03 -5.87896900e-03\n",
      "  3.11825150e-02  3.03341400e-02 -2.61861600e-02 -5.88117770e-03\n",
      "  2.92576270e-02 -8.79664300e-03 -6.72575600e-04  8.57411400e-03\n",
      "  1.02752090e-02  7.83699300e-02  1.53310910e-02 -2.04092700e-02\n",
      "  7.97846100e-03  8.94164800e-02  1.79935170e-02  3.09623180e-02\n",
      " -2.07985840e-02 -9.56556700e-03  8.48388800e-04  2.90961170e-02\n",
      " -3.28140670e-02 -1.86976750e-02 -3.84472870e-02 -1.06045730e-02\n",
      " -1.26770600e-03  1.30197280e-02  8.82673900e-03 -7.19977780e-03\n",
      " -2.18173250e-02 -1.68278010e-02  8.28140000e-03  4.72966400e-02\n",
      " -1.30575870e-02 -1.48566480e-02 -8.93005200e-03  9.29755900e-02\n",
      "  1.53338890e-02 -4.89799300e-02 -3.58934900e-02  1.14311190e-02\n",
      " -4.31151200e-03  1.99258600e-02  5.56444970e-02  2.23119690e-02\n",
      "  2.75083450e-02  2.32134130e-02  1.20724170e-02 -6.24749160e-03\n",
      " -1.54073825e-02  3.60724200e-02  6.31121700e-03 -1.37870920e-02\n",
      " -2.75687600e-02 -4.74906800e-03 -2.67297550e-02  8.23233400e-03\n",
      "  6.97879800e-02 -2.21484900e-02  1.62307040e-02 -1.38501540e-02\n",
      " -5.68806300e-02 -4.46342450e-02 -1.32073825e-02 -1.16050140e-02\n",
      "  7.91499600e-02  1.30979460e-02  5.14040330e-02  4.42372970e-02\n",
      "  3.44134830e-02  2.94029490e-02 -3.19519940e-02  9.14426700e-03\n",
      "  1.17792790e-02  3.45445650e-02 -4.37979740e-02 -3.10944600e-02\n",
      "  1.68956830e-02 -7.50982000e-03  4.31524000e-02 -2.28791140e-02\n",
      "  1.08712560e-02  4.45370530e-02  1.64761860e-02 -2.47275570e-02\n",
      " -9.49818500e-03  1.97807360e-02  1.07859080e-02 -2.49580130e-02\n",
      "  3.10525170e-02  3.93858330e-02  5.04563200e-03  3.65637500e-02\n",
      "  4.66638020e-02  2.82829810e-02 -2.04903240e-02 -1.04000470e-02\n",
      "  4.96630780e-02 -2.31794920e-03 -8.89016300e-03  1.06043510e-02\n",
      " -1.23483130e-02  4.53389100e-02 -6.12357170e-03  3.04061040e-02\n",
      "  6.35665300e-03  2.49276660e-02  1.09894900e-03  1.77550860e-02\n",
      " -2.37923660e-02  3.34461780e-02  3.32315620e-03  9.49597200e-03\n",
      "  3.23168900e-02 -2.33116540e-02 -2.89210880e-02  3.25214860e-03\n",
      " -4.25559880e-02  4.16670630e-02  3.34617230e-02  6.73307850e-03\n",
      "  7.76135500e-02 -2.81212550e-02 -1.06133600e-02 -1.05769150e-02\n",
      " -3.03423420e-02 -2.82740820e-02 -3.18388160e-02  2.39622020e-02\n",
      " -5.03215040e-02 -6.41078500e-02  6.05930500e-03  5.40620500e-03\n",
      " -3.47626260e-03 -3.18080600e-02 -2.80358470e-02  5.84958870e-03\n",
      "  2.61878520e-02  2.11703850e-02 -3.29771100e-02  4.87782880e-02\n",
      "  1.44808680e-02  1.65752060e-02 -3.18444860e-02  4.31355280e-02\n",
      "  2.00444070e-02 -1.20004850e-02  1.47897120e-02 -3.24050600e-02\n",
      " -1.91783420e-02  1.07096640e-02  2.28138040e-02 -8.03429040e-02\n",
      "  2.79713590e-03  4.45923100e-03  1.29707070e-02  2.01113970e-02\n",
      "  2.28867340e-02  1.27533940e-02  4.12376500e-02  2.96619800e-02\n",
      "  3.81218700e-02  2.81362740e-02  3.21363430e-02  9.91808500e-03\n",
      "  2.21930180e-02 -9.89323700e-04 -3.48194760e-02 -9.17077200e-03\n",
      "  1.12919800e-02  4.54853550e-02 -1.43925855e-02 -2.95374610e-03\n",
      "  1.04575130e-02  8.50501100e-03  1.20757600e-02 -1.79078110e-02\n",
      " -2.81654500e-02  3.48492900e-03 -3.28315400e-02 -3.60283600e-02\n",
      " -2.62415860e-02 -1.23788940e-02 -1.16214720e-02 -6.63371150e-03\n",
      "  5.08203540e-03  3.21828240e-02 -1.19049990e-02 -1.44675495e-02\n",
      " -2.68000290e-02  6.27341500e-03  6.92719440e-02 -5.94213930e-03\n",
      " -3.62109700e-02 -2.76387520e-02  7.07317840e-03 -2.39688300e-02\n",
      "  1.61587910e-02  3.15245280e-02  6.42158000e-02 -3.09710990e-02\n",
      " -2.46254910e-02 -4.57305460e-02 -1.20908170e-02 -3.55932450e-02\n",
      "  3.21049300e-02 -7.18814100e-02  1.72226900e-02 -4.41817200e-03\n",
      " -1.28722730e-02 -3.24419300e-03  5.83617060e-03 -1.02932100e-03\n",
      " -2.02275010e-02  4.05836800e-02 -4.10485400e-02 -3.93263600e-02\n",
      " -4.27351200e-02  2.36405100e-02  2.92102040e-02  1.13452650e-03\n",
      "  2.26929570e-03  3.54395170e-02  3.09976230e-02 -3.34244070e-02\n",
      " -8.61486000e-03 -3.84778830e-02 -4.07331300e-02 -6.19353800e-03]\n"
     ]
    }
   ],
   "source": [
    "string_to_embed = qwen.get_last_response_text() # \"Hello world!\"\n",
    "\n",
    "embeddinggemma = OllamaClient(\"embeddinggemma\", log_conversations=False)\n",
    "\n",
    "embeddings = np.array(embeddinggemma.generate_embeddings(string_to_embed))\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick similarity sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Similar  | Similarity: 0.6594\n",
      "Similar      | Similarity: 0.8465\n",
      "Identical    | Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "cases = [\n",
    "    (\"Not Similar\", \"The quick brown fox jumps over the lazy dog\", \n",
    "     \"Machine learning is a subset of artificial intelligence\"),\n",
    "    (\"Similar\", \"A dog is playing in the park\", \n",
    "     \"A puppy is running in the garden\"),\n",
    "    (\"Identical\", \"Hello world!\", \"Hello world!\")\n",
    "]\n",
    "\n",
    "for label, text1, text2 in cases:\n",
    "    emb1 = np.array(embeddinggemma.generate_embeddings(text1)).reshape(1, -1)\n",
    "    emb2 = np.array(embeddinggemma.generate_embeddings(text2)).reshape(1, -1)\n",
    "    similarity = cosine_similarity(emb1, emb2)[0][0]\n",
    "    print(f\"{label:12s} | Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test env setup below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call boilerplate code and specific constructs you need from your src files here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telephone Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test env setup below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call boilerplate code and specific constructs you need from your src files here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualization and Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test env setup below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call boilerplate code and specific constructs you need from your src files here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
