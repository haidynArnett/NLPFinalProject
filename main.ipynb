{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7RTwSFFAEnV"
      },
      "source": [
        "# NLP Final Project Notebook\n",
        "\n",
        "This notebook contains the main analysis and experiments for the NLP final project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbQiD1JDAEnY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncomment and run the cell below in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Replace this by the folder path where you put your assignment. If you are working with local, skip executing the previous cell and add the path to local directory here.\n",
        "# # folder_path = '.'\n",
        "# import os\n",
        "# folder_path = '/content/drive/My Drive/Colab Notebooks/NLPFinalProject-output_test' # Change the path to the folder where the assignment is stored in Google Drive.\n",
        "\n",
        "# # Files in the folder -\n",
        "# os.listdir(folder_path)\n",
        "\n",
        "# os.chdir(folder_path)\n",
        "\n",
        "# print(os.listdir())\n",
        "\n",
        "# print('Current working directory -', os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import importlib, sys\n",
        "# sys.modules[\"imp\"] = importlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from importlib import reload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install components\n",
        "# !curl https://ollama.ai/install.sh | sh\n",
        "# !pip install ollama\n",
        "\n",
        "# !echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "\n",
        "# import os\n",
        "# # Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "# os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "# # Start server\n",
        "# import subprocess\n",
        "# proccess = subprocess.Popen(['ollama', 'serve'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1V4rw_OAEnY"
      },
      "source": [
        "FIRST... Follow instructions in README.md\n",
        "\n",
        "Check if Ollama is properly installed and running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2axug-uDAEnY"
      },
      "outputs": [],
      "source": [
        "!./scripts/check_ollama.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2o8XLY1AEnZ"
      },
      "source": [
        "## Env Example\n",
        "\n",
        "Below is an example cell on how to reference and call functions we create in isolated files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP4ULS3wAEnZ"
      },
      "outputs": [],
      "source": [
        "from src.visualization.utils import example_plot, example_function\n",
        "\n",
        "example_plot()\n",
        "example_function()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjBtlWX1AEna"
      },
      "source": [
        "# Global Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSIutmhdAEna"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom Imports\n",
        "from src.io.ollama_client import OllamaClient\n",
        "from src.io.ollama_client import ConversationEntry, SavedConversationData\n",
        "\n",
        "# Our function imports\n",
        "from src.experiments.output_task import OutputTest\n",
        "from src.experiments.telephone import TelephoneTest\n",
        "from src.metrics.similarity import compute_cosine_similarity_matrix\n",
        "from src.metrics.drift import velocity, acceleration, normalize_0_1, ema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcq4RCElAEna"
      },
      "source": [
        "# Example Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5FQh8AwAEna"
      },
      "source": [
        "## OllamaClient Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc-N1wsiAEna"
      },
      "source": [
        "This is the client that wraps the usage of the ollama API, and should be instantiated per model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMUMyy9JAEnb"
      },
      "outputs": [],
      "source": [
        "qwen = OllamaClient(\"qwen3:0.6b\", log_conversations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmBP1E8JAEnb"
      },
      "source": [
        "Run the cell below to generate text from the model, this will generate a standalone response (no context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKKqbNyyAEnb"
      },
      "outputs": [],
      "source": [
        "qwen.generate_text(\"Hello, world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRmlu5uOAEnb"
      },
      "source": [
        "After running the cell above, the conversation history will be populated with the prompt and response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rZ8ITH5AEnb"
      },
      "outputs": [],
      "source": [
        "print(qwen.get_conversation_count())\n",
        "# The conversation history can be accessed by calling the get_conversation_history method\n",
        "print(qwen.get_conversation_history())\n",
        "# The last conversation can be accessed by calling the get_last_conversation method\n",
        "print(qwen.get_last_conversation())\n",
        "# The conversation history can be cleared by calling the clear_conversation_history method\n",
        "# qwen.clear_conversation_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pubouzCAEnb"
      },
      "source": [
        "## Conversation Persistence\n",
        "\n",
        "You can save and load conversation histories to disk for experiment tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "h50ND8_FAEnc",
        "outputId": "31f58ca8-eebf-4f81-dc10-7771f41ab2e1"
      },
      "outputs": [],
      "source": [
        "# Create a client with custom output directory\n",
        "client = OllamaClient(\"qwen3:0.6b\", output_dir=\"./experiments\", log_conversations=True)\n",
        "\n",
        "# Generate some conversations\n",
        "response1 = client.generate_text(\"What is machine learning? Explain like I'm 5 in 20 words or less.\")\n",
        "print(f\"Response 1: {response1}\\n\")\n",
        "\n",
        "response2 = client.generate_text(\"Explain neural networks? Explain like I'm 5 in 20 words or less.\")\n",
        "print(f\"Response 2: {response2}\\n\")\n",
        "\n",
        "# Flush conversation history to disk\n",
        "saved_path = client.flush_conversation_history(\"test_experiment\")\n",
        "print(f\"Saved conversation history to: {saved_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BLp6cjzAEnc"
      },
      "source": [
        "### Loading Conversation History\n",
        "\n",
        "You can reload conversation histories from disk. By default, it loads the most recent file.\n",
        "\n",
        "- Method 1: Using get_conversation_history() to iterate through the in-memory loaded conversations\n",
        "- Method 2: Using iter_experiment_conversations() to iterate directly from files (memory-efficient for large experiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vop52jSYAEnc"
      },
      "outputs": [],
      "source": [
        "new_client = OllamaClient(\"qwen3:0.6b\", output_dir=\"./experiments\", log_conversations=True)\n",
        "\n",
        "# Load the most recent conversation history for \"test_experiment\"\n",
        "count = new_client.load_conversation_history(\"test_experiment\")\n",
        "print(f\"Loaded {count} conversations from disk\\n\")\n",
        "\n",
        "# Verify the loaded history\n",
        "print(f\"Conversation count: {new_client.get_conversation_count()}\")\n",
        "print(f\"Last response: {new_client.get_last_response_text()}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Method 1: Iterate through in-memory conversation history\n",
        "# print(\"METHOD 1: Iterating through in-memory conversation history\")\n",
        "# print(\"=\"*80)\n",
        "# for i, conv in enumerate[ConversationEntry](new_client.get_conversation_history(), 1):\n",
        "#     print(f\"\\n--- Conversation {i} ---\")\n",
        "#     print(f\"Prompt:   {conv['input'][:80]}...\")  # First 80 chars\n",
        "#     print(f\"Response: {conv['response'][:80]}...\")  # First 80 chars\n",
        "\n",
        "# print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Method 2: Memory-efficient file iterator (doesn't load all into memory)\n",
        "# print(\"METHOD 2: Iterating using file iterator (memory-efficient)\")\n",
        "# print(\"=\"*80)\n",
        "# for i, conv in enumerate[ConversationEntry](new_client.iter_experiment_conversations(\"test_experiment\"), 1):\n",
        "#     print(f\"\\n--- Conversation {i} ---\")\n",
        "#     print(f\"Prompt:   {conv['input'][:80]}...\")  # First 80 chars\n",
        "#     print(f\"Response: {conv['response'][:80]}...\")  # First 80 chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13TD1TWQAEnc"
      },
      "source": [
        "### Viewing Saved JSON Structure\n",
        "\n",
        "The saved conversation history contains all the details needed for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH4lgKgMAEnc"
      },
      "outputs": [],
      "source": [
        "# Example of viewing a single conversation entry\n",
        "conversation = new_client.get_conversation_history()[0]\n",
        "\n",
        "print(\"Conversation Structure:\")\n",
        "print(f\"Input: {conversation['input']}\")\n",
        "print(f\"Response: {conversation['response']}\")\n",
        "print(f\"Type: {conversation['type']}\")\n",
        "print(f\"Timestamp: {conversation['timestamp']}\")\n",
        "print(f\"\\nDetails keys: {conversation['details'].keys()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCK6xFsJAEnd"
      },
      "source": [
        "The response from ollama and stored in the client contains a ton of information if needed. Note the difference in the already parsed thinking and response secitons, this could be used later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtT6wgNxAEnd"
      },
      "outputs": [],
      "source": [
        "resp = qwen.get_last_conversation().get(\"details\")\n",
        "print(json.dumps(resp, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHgUjiA-AEnd"
      },
      "source": [
        "Or more simply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-agoxpitAEnd"
      },
      "outputs": [],
      "source": [
        "qwen.get_last_response_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVELOscAEnd"
      },
      "source": [
        "Depending on how you choose to setup your experiments, you can choose to use the cached responses in the client class, or keep track of the inputs and outputs outside of the client, either is fine just use `log_conversations=False` to prevent memory bloat if you are doing it yourself. I imagine keeping it within the class will work better, then appending a new message like `prompt + qwen.get_last_response()` for the output test. This way, we can parse the message history into formats used for calculating embeddings, drift, accelertaion etc. Use the flush functionality to save to the file system if you are running this for a while, or running this for a lot of iterations. The scheme for your tests should be split into data creation, then data parsing. I.E. create a cell to run the prompts and save the conversations, then create a cell that iterates through the class or files to compute embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_RRhjXtAEnd"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUVp9nc-AEne"
      },
      "source": [
        "Create embeddings using the same OllamaClient with a new model name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1sKzOd5AEne"
      },
      "outputs": [],
      "source": [
        "string_to_embed = qwen.get_last_response_text() # \"Hello world!\"\n",
        "\n",
        "embeddinggemma = OllamaClient(\"embeddinggemma\", log_conversations=False)\n",
        "\n",
        "embeddings = np.array(embeddinggemma.generate_embeddings(string_to_embed))\n",
        "\n",
        "print(embeddings.shape)\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ2wSYmJAEne"
      },
      "source": [
        "A quick similarity sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3viRLPnpAEne"
      },
      "outputs": [],
      "source": [
        "# Test cases\n",
        "cases = [\n",
        "    (\"Not Similar\", \"The quick brown fox jumps over the lazy dog\",\n",
        "     \"Machine learning is a subset of artificial intelligence\"),\n",
        "    (\"Similar\", \"A dog is playing in the park\",\n",
        "     \"A puppy is running in the garden\"),\n",
        "    (\"Identical\", \"Hello world!\", \"Hello world!\")\n",
        "]\n",
        "\n",
        "for label, text1, text2 in cases:\n",
        "    emb1 = np.array(embeddinggemma.generate_embeddings(text1)).reshape(1, -1)\n",
        "    emb2 = np.array(embeddinggemma.generate_embeddings(text2)).reshape(1, -1)\n",
        "    similarity = cosine_similarity(emb1, emb2)[0][0]\n",
        "    print(f\"{label:12s} | Similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7m9bqZxpTJA"
      },
      "source": [
        "A quick drift sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2f41jCdpUFp"
      },
      "outputs": [],
      "source": [
        "linear = np.linspace(1.0, 0.0, 10)             # steadily declining\n",
        "quadratic = 1 - (np.linspace(0, 1, 10) ** 2)   # faster decline at the start\n",
        "plateau = np.concatenate([np.ones(5), np.linspace(1, 0.5, 5)])  # flat then drop\n",
        "\n",
        "cases = {\"linear\": linear, \"quadratic\": quadratic, \"plateau\": plateau}\n",
        "\n",
        "for name, s in cases.items():\n",
        "    v = velocity(s)\n",
        "    a = acceleration(s)\n",
        "    norm = normalize_0_1(s)\n",
        "    smooth = ema(norm, alpha=0.3)\n",
        "\n",
        "    print(f\"\\n=== {name.upper()} SEQUENCE ===\")\n",
        "    print(\"Original:\", np.round(s, 3))\n",
        "    print(\"Velocity:\", np.round(v, 3))\n",
        "    print(\"Acceleration:\", np.round(a, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iahIeFcCAEne"
      },
      "source": [
        "# Output Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rED8ZpyqAEne"
      },
      "source": [
        "Create test env setup below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DCMyO0uAEnf"
      },
      "outputs": [],
      "source": [
        "# Call boilerplate code and specific constructs you need from your src files here\n",
        "output_test_qwen = OllamaClient(\"qwen3:0.6b\", output_dir=\"./experiments\", log_conversations=True)\n",
        "output_test = OutputTest(output_test_qwen)\n",
        "output_test.run(50, save_history_every=50, experiment_name=\"output_task_qwen3:0.6b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.visualization.utils import visualize_output_test\n",
        "visualize_output_test(\"output_task_qwen3:0.6b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run average batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.experiments.output_task import run_batch_output_test\n",
        "\n",
        "# Batch Output Test - Run multiple times\n",
        "output_test_qwen_batch = OllamaClient(\"qwen3:0.6b\", output_dir=\"./output_task_experiments\", log_conversations=True)\n",
        "output_test_gemma_batch = OllamaClient(\"gemma3:1b-it-q8_0\", output_dir=\"./output_task_experiments\", log_conversations=True)\n",
        "output_test_llama_batch = OllamaClient(\"llama3.2:1b-instruct-q4_K_M\", output_dir=\"./output_task_experiments\", log_conversations=True)\n",
        "\n",
        "run_batch_output_test(\n",
        "    clients=[output_test_qwen_batch, output_test_gemma_batch, output_test_llama_batch],\n",
        "    num_runs=30,\n",
        "    iterations_per_run=15,\n",
        "    save_history_every=15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.visualization.utils import visualize_output_test_aggregated\n",
        "visualize_output_test_aggregated(\"output_task_llama3.2:1b-instruct-q4_K_M\", show_individual_runs=True, output_dir=\"./output_task_experiments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.visualization.utils import visualize_output_test_comparison\n",
        "visualize_output_test_comparison(output_dir=\"./output_task_experiments\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GPhDXckAEnf"
      },
      "source": [
        "# Telephone Test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30-EjChFAEnf"
      },
      "source": [
        "Create test env setup below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "telephone_test_qwen06_batch = OllamaClient(\"qwen3:0.6b\", output_dir=\"./telephone_task_experiments\", log_conversations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "telephone_test_qwen8_batch = OllamaClient(\"qwen3:8b\", output_dir=\"./telephone_task_experiments\", log_conversations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# telephone_test_qwen235_batch = OllamaClient(\"qwen3:235b\", output_dir=\"./telephone_task_experiments\", log_conversations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "telephone_test_llama70_batch = OllamaClient(\"llama3.1:70b\", output_dir=\"./telephone_task_experiments\", log_conversations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "telephone_test_llama8_batch = OllamaClient(\"llama3.1:8b\", output_dir=\"./telephone_task_experiments\", log_conversations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "telephone_test_gemma4_batch = OllamaClient(\"Gemma3:4b\", output_dir=\"./telephone_task_experiments\", log_conversations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "telephone_test_gptoss_batch = OllamaClient(\"GPT-OSS:20b\", output_dir=\"./telephone_task_experiments\", log_conversations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXVYdFcPAEnf"
      },
      "outputs": [],
      "source": [
        "from src.experiments.telephone import run_batch_telephone_test\n",
        "# Define shared embedding client\n",
        "embedding_model = OllamaClient(\"embeddinggemma\", output_dir=\"./telephone_task_experiments\", log_conversations=False)\n",
        "\n",
        "# Run batch test (uses DEFAULT_INITIAL_TEXT from telephone.py)\n",
        "run_batch_telephone_test(\n",
        "    text_clients=[telephone_test_qwen06_batch, telephone_test_qwen8_batch, telephone_test_llama70_batch, telephone_test_llama8_batch, telephone_test_gemma4_batch, telephone_test_gptoss_batch],\n",
        "    embedding_client=embedding_model,\n",
        "    num_runs=300,\n",
        "    iterations_per_run=20,\n",
        "    save_history_every=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqSD_4z0AEng"
      },
      "outputs": [],
      "source": [
        "from src.visualization.utils import visualize_telephone_test_aggregated\n",
        "\n",
        "# Visualize one model's aggregated runs\n",
        "visualize_telephone_test_aggregated(\n",
        "    experiment_name=\"telephone_qwen3:0.6b\",\n",
        "    output_dir=\"./telephone_task_experiments\",\n",
        "    show_individual_runs=True,\n",
        "    confidence_level=0.95\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.visualization.utils import visualize_telephone_test_comparison\n",
        "\n",
        "# Auto-discovers and compares all telephone_* experiments\n",
        "visualize_telephone_test_comparison(\n",
        "    output_dir=\"./telephone_task_experiments\",\n",
        "    confidence_level=0.95\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrsJl_dbAEng"
      },
      "source": [
        "# Vizualization and Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_bdIt2PAEng"
      },
      "source": [
        "Create test env setup below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8OaMZyCAEng"
      },
      "outputs": [],
      "source": [
        "# Call boilerplate code and specific constructs you need from your src files here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp-final",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
